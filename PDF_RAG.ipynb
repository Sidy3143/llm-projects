{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNU7YaoHYvWEaZ7ljkFFx0S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidy3143/llm-projects/blob/main/PDF_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fsspec==2025.3.0 gcsfs transformers accelerate peft bitsandbytes datasets trl"
      ],
      "metadata": {
        "id": "itn2R9WD2xz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e0fff0-c89d-4bdc-fd82-9b364a7b44d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m366.4/366.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "19Bh75878XGd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
        "    bnb_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "3gz9Mzvn8e2q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBqiY-uIJlLV",
        "outputId": "1ea2bb9e-803b-41ee-f956-fc50a65fd34e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "The token `mistral` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `mistral`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\":0},\n",
        ")"
      ],
      "metadata": {
        "id": "bKcKeItEOCI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4bclkJ2yOGPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fp2UWiDlbnt9",
        "outputId": "330bc336-ac7a-4420-fe8a-452994b62b4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d43dea52-26a3-4327-a726-c20216ebd8e1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d43dea52-26a3-4327-a726-c20216ebd8e1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Test Time Scaling Paper.pdf to Test Time Scaling Paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUxoVyxsLjRU",
        "outputId": "12e821f1-8f86-4238-d137-e9a181e42561"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def load_pdf(file_name):\n",
        "  with open(file_name, 'rb') as f:\n",
        "    reader = PyPDF2.PdfReader(f)\n",
        "    text = ''\n",
        "    for page in reader.pages:\n",
        "      text += page.extract_text() + '\\n'\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "zKwiWzdEb3wT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = list(uploaded.keys())[0]\n",
        "document_text = load_pdf(file_name)"
      ],
      "metadata": {
        "id": "680d2SZjcuBZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install llama-index\n",
        "!pip -q install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHqMLzNdnnNQ",
        "outputId": "b7ea759b-103e-4924-e81e-f389a2a82c65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/7.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.6/7.7 MB\u001b[0m \u001b[31m133.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/266.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.schema import Document\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "aD106H3mo86M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "Settings.llm = None\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 25"
      ],
      "metadata": {
        "id": "A0bOkgAnOJCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "RKhCgYnoukZW",
        "outputId": "e9632737-675e-465d-a17b-5700161487dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2024-8-7\\nScaling LLM Test-Time Compute Optimally can\\nbe More Effective than Scaling Model Parameters\\nCharlie Snellâ™¦, 1, Jaehoon Lee2, Kelvin Xuâ™£, 2and Aviral Kumarâ™£, 2\\nâ™£Equal advising,1UC Berkeley,2Google DeepMind,â™¦Work done during an internship at Google DeepMind\\nEnabling LLMs to improve their outputs by using more test-time computation is a critical step towards\\nbuilding generally self-improving agents that can operate on open-ended natural language. In this paper,\\nwe study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an\\nLLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its\\nperformance on a challenging prompt? Answering this question has implications not only on the achievable\\nperformance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time\\nand pre-training compute. Despite its importance, little research attempted to understand the scaling\\nbehaviors of various test-time inference methods. Moreover, current work largely provides negative results\\nfor a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time\\ncomputation: (1) searching against dense, process-based verifier reward models; and (2) updating the\\nmodelâ€™s distribution over a response adaptively, given the prompt at test time. We find that in both cases, the\\neffectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty\\nof the prompt. This observation motivates applying a â€œcompute-optimalâ€ scaling strategy, which acts to\\nmost effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we\\ncan improve the efficiency of test-time compute scaling by more than 4 Ã—compared to a best-of-N baseline.\\nAdditionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains\\nsomewhat non-trivial success rates, test-time compute can be used to outperform a 14 Ã—larger model.\\n1. Introduction\\nHumans tend to think for longer on difficult problems to reliably improve their decisions [ 9,17,18].\\nCan we instill a similar capability into todayâ€™s large language models (LLMs)? More specifically, given\\na challenging input query, can we enable language models to most effectively make use of additional\\ncomputation at test time so as to improve the accuracy of their response? In theory, by applying additional\\ncomputation at test time, an LLM should be able to do better than what it was trained to do. In addition,\\nsuch a capability at test-time also has the potential to unlock new avenues in agentic and reasoning\\ntasks [28,34,47]. For instance, if pre-trained model size can be traded off for additional computation\\nduring inference, this would enable LLM deployment in use-cases where smaller on-device models could\\nbe used in place of datacenter scale LLMs. Automating the generation of improved model outputs by\\nusing additional inference-time computation also provides a path towards a general self-improvement\\nalgorithm that can function with reduced human supervision.\\nPrior work studying inference-time computation provides mixed results. On the one hand, some works\\nshow that current LLMs can use test-time computation to improve their outputs [ 4,8,23,30,48], on\\nthe other hand, other work shows that the effectiveness of these methods on more complex tasks such\\nas math reasoning remains highly limited [ 15,37,43], even though reasoning problems often require\\ndrawing inferences about existing knowledge as opposed to new knowledge. These sorts of conflicting\\nfindings motivate the need for a systematic analysis of different approaches for scaling test-time compute.\\nCorresponding author(s): csnell22@berkeley.eduarXiv:2408.03314v1  [cs.LG]  6 Aug 2024\\n21232527\\nGeneration Budget202530354045MATH Accuracy (%)\\nCompute Optimal Revisions\\nMajority\\nBest-of-N Weighted\\nCompute Optimal\\nParallel\\n<<1 ~=1 >>1\\nRatio of Inference Tokens to Pretraining Tokens40\\n30\\n20\\n10\\n0102030Relative Improvement in Accuracy\\nFrom Test-time Compute (%)+21.6%\\n+16.7%\\n+5.4%+27.8%\\n+3.5%\\n-24.3%+11.8%\\n-11.9%\\n-37.2%Comparing Test-time and Pretraining Compute\\nin a FLOPs Matched Evauation\\nEasy Questions\\nMedium Questions\\nHard QuestionsIteratively Revising Answers at Test-time\\n21\\n23\\n25\\n27\\n29\\nGeneration Budget1015202530354045MATH Accuracy (%)\\nCompute Optimal Search\\nMajority\\nORM Best-of-N Weighted\\nPRM Best-of-N Weighted\\nPRM Compute Optimal\\n<<1 ~=1 >>1\\nRatio of Inference Tokens to Pretraining Tokens50\\n40\\n30\\n20\\n10\\n01020Relative Improvement in Accuracy\\nFrom Test-time Compute (%)+19.1%\\n+2.2% +2.0%\\n-5.6%\\n-35.6%-30.6%0.0%\\n-35.3%\\n-52.9%Comparing Test-time and Pretraining Compute\\nin a FLOPs Matched Evauation\\nEasy Questions\\nMedium Questions\\nHard QuestionsTest-time Search Against a PRM VerifierFigure 1 âˆ£Summary of our main results. Left: Compute-optimal scaling for iterative self-refinement (i.e., revisions) and search. On\\nthe left, we compare the compute-optimal scaling policy for our PaLM 2-S* revision model against baselines in the revision setting (top) and the\\nPRM search setting (bottom). We see that in the revisions case, the gap between standard best-of-N (e.g. â€œparallelâ€) and compute-optimal\\nscaling gradually widens, enabling compute-optimal scaling to outperform best-of-N with 4Ã—less test-time compute. Similarly, in the PRM\\nsearch setting, we observe significant early improvements over best-of-N from compute-optimal scaling, nearly outperforming best-of-N with 4Ã—\\nless compute at points. See Sections 5 and 6 for details. Right: Comparing test-time compute and model parameter scaling. We compare\\nthe performance of compute-optimal test-time scaling with PaLM 2-S* against the performance of a âˆ¼14Ã—larger pretrained model without\\nadditional test-time compute (e.g. greedy sampling). We consider the setting where we expect ğ‘‹tokens of pretraining for both models and ğ‘Œ\\ntokens of inference. By training a larger model, we effectively multiply the FLOPs requirement for both of these terms. If we were to apply\\nadditional test-time compute with the smaller model, so as to match this larger modelâ€™s FLOPs requirement, how would it compare in terms\\nof accuracy? We see that for the revisions (top) when ğ‘Œ<<ğ‘‹, test-time compute is often preferable to additional pretraining. However, as\\nthe inference to pretraining token ratio increases, test-time compute remains preferable on easy questions. Whereas on harder questions,\\npretraining is preferable in these settings. We also see a similar trend with PRM search (bottom). See Section 7 for more details.\\nWe are interested in understanding the benefits of scaling up test-time compute. Arguably the simplest\\nand most well-studied approach for scaling test-time computation is best-of-N sampling: sampling N\\noutputs in â€œparallelâ€ from a base LLM and selecting the one that scores the highest per a learned verifier\\nor a reward model [ 7,22]. However, this approach is not the only way to use test-time compute to\\nimprove LLMs. By modifying either the proposal distribution from which responses are obtained (for\\ninstance, by asking the base model to revise its original responses â€œsequentiallyâ€ [ 28]) or by altering how\\ntheverifieris used (e.g. by training a process-based dense verifier [ 22,45] and searching against this\\nverifier), the ability scale test-time compute could be greatly improved, as we show in the paper.\\nTo understand the benefits of scaling up test-time computation, we carry out experiments on the\\nchallenging MATH [ 13] benchmark using PaLM-2 [ 3] models specifically fine-tuned1to either revise\\n1Capability-specific finetuning is necessary to induce revision and verification capabilities into the base model on MATH\\n2\\nincorrect answers [28] (e.g. improving the proposal distribution; Section 6) or verify the correctness of\\nindividual steps in an answer using a process-based reward model (PRM) [ 22,45] (Section 5). With\\nboth approaches, we find that the efficacy of a particular test-time compute strategy depends critically\\non both the nature of the specific problem at hand and the base LLM used. For example, on easier\\nproblems, for which the base LLM can already readily produce reasonable responses, allowing the model\\nto iteratively refine its initial answer by predicting a sequence of N revisions (i.e., modifying the proposal\\ndistribution), may be a more effective use of test-time compute than sampling N independent responses in\\nparallel. On the other hand, with more difficult problems that may require searching over many different\\nhigh-level approaches to solving the problem, re-sampling new responses independently in parallel or\\ndeploying tree-search against a process-based reward model is likely a more effective way to use test-time\\ncomputation. This finding illustrates the need to deploy an adaptive â€œcompute-optimalâ€œ strategy for\\nscaling test-time compute , wherein the specific approach for utilizing test-time compute is selected\\ndepending on the prompt, so as to make the best use of additional computation. We also show that a\\nnotion of question difficulty (Section 4) from the perspective of the base LLM can be used to predict the\\nefficacy of test-time computation, enabling us to practically instantiate this â€˜compute-optimalâ€™ strategy\\ngiven a prompt. By appropriately allocating test-time compute in this way, we are able to greatly improve\\ntest-time compute scaling, surpassing the performance of a best-of-N baseline while only using about 4x\\nless computation with both revisions and search (Sections 5 and 6).\\nUsing our improved test-time compute scaling strategy, we then aim to understand to what extent\\ntest-time computation can effectively substitute for additional pretraining. We conduct a FLOPs-matched\\ncomparison between a smaller model with additional test-time compute and pretraining a 14x larger\\nmodel. We find that on easy and intermediate questions, and even hard questions (depending on the\\nspecific conditions on the pretraining and inference workload), additional test-time compute is often\\npreferable to scaling pretraining. This finding suggests that rather than focusing purely on scaling\\npretraining, in some settings it is be more effective to pretrain smaller models with less compute,\\nand then apply test-time compute to improve model outputs . That said, with the most challenging\\nquestions, we observe very little benefits from scaling up test-time compute. Instead, we find that\\non these questions, it is more effective to make progress by applying additional pretraining compute,\\ndemonstrating that current approaches to scaling test-time compute may not be 1-to-1 exchangeable\\nwith scaling pretraining. Overall, this suggests that even with a fairly naÃ¯ve methodology, scaling up\\ntest-time computation can already serve to be more preferable to scaling up pretraining, with only more\\nimprovements to be attained as test-time strategies mature. Longer term, this hints at a future where\\nfewer FLOPs are spent during pretraining and more FLOPs are spent at inference.\\n2. A Unified Perspective on Test-Time Computation: Proposer and Verifier\\nWe first unify approaches for using test-time computation and then analyze some representative methods.\\nFirst, we view the use of additional test-time compute through the lens of modifying the modelâ€™s predicted\\ndistribution adaptively at test-time, conditioned on a given prompt. Ideally, test-time compute should\\nmodify the distribution so as to generate better outputs than naÃ¯vely sampling from the LLM itself would.\\nIn general, there are two knobs to induce modifications to an LLMâ€™s distribution: (1) at the input level :\\nsince these capabilities are absent even in strong proprietary LLMs [ 15,33]. However, we expect that future LLMs will be more\\neffective at verification and revision due to both increased scale and the inclusion of additional data targeted specifically towards\\nthese capabilities [ 5,24,36]. Therefore in order to make progress towards understanding scaling of test-time computation,\\nwe must use models finetuned for these capabilities. That said, we expect future models to be pretrained for such capabilities\\ndirectly, therefore avoiding the need for capability-specific finetuning.\\n3\\nby augmenting the given prompt with an additional set of tokens that the LLM conditions on to obtain\\nthe modified distribution, or (2) at the output level : by sampling multiple candidates from the standard\\nLM and performing surgery on these candidates. In other words, we could either modify the proposal\\ndistribution induced by the LLM itself such that it is an improvement over naÃ¯vely conditioning on the\\nprompt or we could use some post-hoc verifiers or scorers to perform output modifications. This process\\nis reminiscent of Markov chain Monte Carlo (MCMC) [ 2] sampling from a complex target distribution but\\nby combining a simple proposal distribution and a score function. Modifying the proposal distribution\\ndirectly by altering input tokens and using a verifier form two independent axes of our study.\\nModifyingtheproposaldistribution. Onewaytoimprovetheproposaldistributionistodirectlyoptimize\\nthe model for a given reasoning task via RL-inspired finetuning methods such as STaR or ReSTEM[35,50].\\nNote that these techniques do not utilize any additional input tokens but specifically finetune the model to\\ninduce an improved proposal distribution. Instead, techniques such as self-critique [ 4,8,23,30] enable\\nthe model itself to improve its own proposal distribution at test time by instructing it to critique and\\nrevise its own outputs in an iterative fashion. Since prompting off-the-shelf models is not effective at\\nenabling effective revisions at test time, we specifically finetune models to iteratively revise their answers\\nin complex reasoning-based settings. To do so, we utilize the approach of finetuning on on-policy data\\nwith Best-of-N guided improvements to the model response [28].\\nOptimizing the verifier. In our abstraction of the proposal distribution and verifier, the verifier is used to\\naggregate or select the best answer from the proposal distribution. The most canonical way to use such\\na verifier is by applying best-of-N sampling, wherein we sample N complete solutions and then select\\nthe best one according to a verifier [ 7]. However, this approach can be further improved by training\\na process-based verifier [ 22], or a process reward model (PRM), which produces a prediction of the\\ncorrectness of each intermediate step in an solution, rather than just the final answer. We can then utilize\\nthese per-step predictions to perform tree search over the space of solutions, enabling a potentially more\\nefficient and effective way to search against a verifier, compared to naÃ¯ve best-of-N [6, 10, 48].\\n3. How to Scale Test-Time Computation Optimally\\nGiven the unification of various methods, we would now like to understand how to most effectively utilize\\ntest-time computation to improve LM performance on a given prompt. Concretely we wish to answer:\\nProblem setup\\nWe are given a prompt and a test-time compute budget within which to solve the problem. Under\\nthe abstraction above, there are different ways to utilize test-time computation. Each of these\\nmethods may be more or less effective depending on the specific problem given. How can we\\ndetermine the most effective way to utilize test-time compute for a given prompt? And how well\\nwould this do against simply utilizing a much bigger pretrained model?\\nWhen either refining the proposal distribution or searching against a verifier, there are several different\\nhyper-parameters that can be adjusted to determine how a test-time compute budget should be allocated.\\nFor example, when using a model finetuned for revisions as the proposal distribution and an ORM as the\\nverifier, we could either spend the full test-time compute budget on generating N independent samples\\nin parallel from the model and then apply best-of-N, or we could sample N revisions in sequence using a\\nrevision model and then select the best answer in the sequence with an ORM, or strike a balance between\\nthese extremes. Intuitively, we might expect â€œeasierâ€ problems to benefit more from revisions, since the\\n4\\nmodelâ€™s initial samples are more likely to be on the right track but may just need further refinement.\\nOn the other hand, challenging problems may require more exploration of different high-level problem\\nsolving strategies, so sampling many times independently in parallel may be preferable in this setting.\\nIn the case of verifiers, we also have the option to choose between different search algorithms (e.g.\\nbeam-search, lookahead-search, best-of-N), each of which may exhibit different properties depending on\\nthe quality of the verifier and proposal distribution at hand. More sophisticated search procedures might\\nbe more useful in harder problems compared to a much simpler best-of-N or majority baseline.\\n3.1. Test-Time Compute-Optimal Scaling Strategy\\nIn general, we would therefore like to select the optimal allocation of our test-time compute budget\\nfor a given problem. To this end, for any given approach of utilizing test-time compute (e.g., revisions\\nand search against a verifier in this paper, various other methods elsewhere), we define the â€œtest-time\\ncompute-optimal scaling strategyâ€ as the strategy that chooses hyperparameters corresponding to\\na given test-time strategy for maximal performance benefits on a given prompt at test time. Formally,\\ndefine Target(ğœƒ,ğ‘,ğ‘)as the distribution over natural language output tokens induced by the model for a\\ngiven prompt ğ‘, using test-time compute hyper-parameters ğœƒ, and a compute budget of ğ‘. We would\\nlike to select the hyper-parameters ğœƒwhich maximize the accuracy of the target distribution for a given\\nproblem. We express this formally as:\\nğœƒâˆ—\\nğ‘,ğ‘âˆ—(ğ‘)(ğ‘)=argmaxğœƒ(Eğ‘¦âˆ¼Target(ğœƒ,ğ‘,ğ‘)[ 1ğ‘¦=ğ‘¦âˆ—(ğ‘)]), (1)\\nwhereğ‘¦âˆ—(ğ‘)denotes the ground-truth correct response for ğ‘, andğœƒâˆ—\\nğ‘,ğ‘¦âˆ—(ğ‘)(ğ‘)represents the test-time\\ncompute-optimal scaling strategy for the problem ğ‘with compute budget ğ‘.\\n3.2. Estimating Question Difficulty for Compute-Optimal Scaling\\nIn order to effectively analyze the test-time scaling properties of the different mechanisms discussed in\\nSection 2 (e.g. the proposal distribution and the verifier), we will prescribe an approximation to this\\noptimal strategy ğœƒâˆ—\\nğ‘,ğ‘¦âˆ—(ğ‘)(ğ‘)as a function of a statistic of a given prompt. This statistic estimates a notion\\nofdifficulty for a given prompt. The compute-optimal strategy is defined as a function of the difficulty of\\nthis prompt. Despite being only an approximate solution to the problem shown in Equation 1, we find\\nthat it can still induce substantial improvements in performance over a baseline strategy of allocating this\\ninference-time compute in an ad-hoc or uniformly-sampled manner.\\nOur estimate of the question difficulty assigns a given question to one of five difficulty levels. We can then\\nuse this discrete difficulty categorization to estimate ğœƒâˆ—\\nğ‘,ğ‘¦âˆ—(ğ‘)(ğ‘)on a validation set for a given test-time\\ncompute budget. We then apply these compute-optimal strategies on the test-set. Concretely, we select\\nthe best performing test-time compute strategy for each difficulty bin independently. In this way, question\\ndifficulty acts as a sufficient statistic of a question when designing the compute-optimal strategy.\\nDefining difficulty of a problem. Following the approach of Lightman et al. [22], we define question\\ndifficulty as a function of a given base LLM. Specifically, we bin the modelâ€™s pass@1 rate â€“ estimated\\nfrom 2048 samples â€“ on each question in the test set into five quantiles, each corresponding to increasing\\ndifficulty levels. We found this notion of model-specific difficulty bins to be more predictive of the efficacy\\nof using test-time compute in contrast to the hand-labeled difficulty bins in the MATH dataset.\\nThat said, we do note that assessing a questionâ€™s difficulty as described above assumes oracle access to a\\nground-truth correctness checking function, which is of course not available upon deployment where we\\n5\\nare only given access to test prompts that we donâ€™t know the answer to. In order to be feasible in practice,\\na compute-optimal scaling strategy conditioned on difficulty needs to first assess difficulty and then\\nutilize the right scaling strategy to solve this problem. Therefore, we approximate the problemâ€™s difficulty\\nvia amodel-predicted notion of difficulty , which performs the same binning procedure over the the\\naveraged final answer score from a learned verifier (and not groundtruth answer correctness checks) on\\nthe same set of 2048 samples per problem. We refer to this setting as model-predicted difficulty and\\nthe setting which relies on the ground-truth correctness as oracle difficulty .\\nWhile model-predicted difficulty removes the need for need knowing the ground truth label, estimating\\ndifficulty in this way still incurs additional computation cost during inference. That said, this one-time\\ninference cost can be subsumed within the cost for actually running an inference-time strategy (e.g.,\\nwhen using a verifier, one could use the same inference computation for also running search). More\\ngenerally, this is akin to exploration-exploitation tradeoff in reinforcement learning: in actual deployment\\nconditions, we must balance the compute spent in assessing difficulty vs applying the most compute-\\noptimal approach. This is a crucial avenue for future work (see Section 8) and our experiments do not\\naccount for this cost largely for simplicity, since our goal is to present some of the first results of what is\\nin fact possible by effectively allocating test-time compute.\\nSo as to avoid confounders with using the same test set for computing difficulty bins and for selecting\\nthe compute-optimal strategy, we use two-fold cross validation on each difficulty bin in the test set. We\\nselect the best-performing strategy according to performance on one fold and then measure performance\\nusing that strategy on the other fold and vice versa, averaging the results of the two test folds.\\n4. Experimental Setup\\nWe first outline our experimental setup for conducting this analysis with multiple verifier design choices\\nand proposal distributions, followed by the analysis results in the subsequent sections.\\nDatasets. We expect test-time compute to be most helpful when models already have all the basic\\nâ€œknowledgeâ€ needed to answer a question, and instead the primary challenge is about drawing (complex)\\ninferences from this knowledge. To this end, we focus on the MATH [13] benchmark, which consists of\\nhigh-school competition level math problems with a range of difficulty levels. For all experiments, we use\\nthe dataset split consisting of 12k train and 500 test questions, used in Lightman et al. [22].\\nModels. We conduct our analysis using the PaLM 2-S* [ 3] (Codey) base model. We believe this model\\nis representative of the capabilities of many contemporary LLMs, and therefore think that our findings\\nlikely transfer to similar models. Most importantly, this model attains a non-trivial performance on MATH\\nand yet has not saturated, so we expect this model to provide a good test-bed for us.\\n5. Scaling Test-Time Compute via Verifiers\\nIn this section we analyze how test-time compute can be scaled by optimizing a verifier, as effectively as\\npossible. To this end, we study different approaches for performing test-time search with process verifiers\\n(PRMs) and analyze the test-time compute scaling properties of these different approaches.\\n5.1. Training Verifiers Amenable to Search\\nPRM training. Originally PRM training [ 22,42] used human crowd-worker labels. While Lightman\\net al.[22]released their PRM training data (i.e., the PRM800k dataset), we found this data to be largely\\n6\\nineffective for us. We found that it was easy to exploit a PRM trained on this dataset via even naÃ¯ve\\nstrategies such as best-of-N sampling. We hypothesize that this is likely a result of the distribution shift\\nbetween the GPT-4 generated samples in their dataset and our PaLM 2 models. Rather than proceeding\\nwith the expensive process of collecting crowd-worker PRM labels for our PaLM 2 models, we instead\\napply the approach of Wang et al. [45]to supervise PRMs without human labels, using estimates of\\nper-step correctness obtained from running Monte Carlo rollouts from each step in the solution. Our\\nPRMâ€™s per-step predictions therefore correspond to value estimates of reward-to-go for the base modelâ€™s\\nsampling policy, similar to recent work [ 31,45]. We also compared to an ORM baseline (Appendix F)\\nbut found that our PRM consistently outperforms the ORM. Hence, all of the search experiments in this\\nsection use a PRM model. Additional details on PRM training are shown in Appendix D.\\nAnswer aggregation. At test time, process-based verifiers can be used to score each individual step in a\\nset of solutions sampled from the base model. In order to select the best-of-N answers with the PRM, we\\nneed a function that can aggregate across all the per-step scores for each answer to determine the best\\ncandidate for the correct answer. To do this, we first aggregate each individual answerâ€™s per-step scores\\nto obtain a final score for the full answer (step-wise aggregation). We then aggregate across answers to\\ndetermine the best answer (inter-answer aggregation). Concretely, we handle step-wise and inter-answer\\naggregation as follows:\\nâ€¢Step-wise aggregation. Rather than aggregating the per-step scores by taking the product or\\nminimum [ 22,45], we instead use the PRMâ€™s prediction at the last step as the full-answer score.\\nWe found this to perform the best out of all aggregation methods we studied (see Appendix E).\\nâ€¢Inter-answer aggregation. We follow Li et al. [21]and apply â€œbest-of-N weightedâ€ selection rather\\nthan standard best-of-N. Best-of-N weighted selection marginalizes the verifierâ€™s correctness scores\\nacross all solutions with the same final answer, selecting final answer with the greatest total sum.\\n5.2. Search Methods Against a PRM\\nWe optimize the PRM at test time via search methods. We study three search approaches that sample\\noutputs from a few-shot prompted base LLM (see Appendix G). An illustration is shown in Figure 2.\\nBest-of-N weighted. We sample N answers independently from the base LLM and then select the best\\nanswer according to the PRMâ€™s final answer judgement.\\nBeam search. Beam search optimizes the PRM by searching over its per-step predictions. Our implemen-\\ntation is similar to BFS-V [ 10,48]. Concretely, we consider a fixed number of beams ğ‘and a beam width\\nğ‘€. We then run the following steps:\\n1. sample ğ‘initial predictions for the first step in the solution\\n2.score the generated steps according to the PRMâ€™s predicted step-wise reward-to-go estimate (which\\nalso corresponds to the total reward from the prefix since the reward is sparse in this setting)\\n3. filter for only the topğ‘\\nğ‘€highest scoring steps\\n4.now from each candidate, sample ğ‘€proposals from the next step, resulting in a total of ğ‘/ğ‘€Ã—ğ‘€\\ncandidate prefixes again. Then repeat steps 2-4 again.\\nWe run this algorithm until the end of a solution or the maximum number of rounds of beam expansion\\nare attained (40 in our case). We conclude the search with N final answer candidates, to which we apply\\nbest-of-N weighted selection described above to make our final answer prediction.\\n7\\n                 \\n                            =   Apply Verifier                             =   Full Solution                             =   Intermediate solution step                        =   Selected by verifier                     =   Rejected by verifier Best-of-N Beam Search Lookahead Search \\nQuestion Select the top-N samples \\nat each step using the \\nPRM Beam search, but at each step \\nrollout k-steps in advance, using \\nthe PRM value at the end of the \\nrollout to represent the value for \\nthe current step \\nPropagate \\nPRM value \\nback to \\nstep\\nContinue Search from \\nthe top-N options \\nâ€¦Select the best final answer using the verifier \\nKey: Select the best final answer using the verifier Generate N full solutions, \\nselecting the best one with the \\nverifier Question Question \\nRollout \\nk-steps Figure 2 âˆ£Comparing different PRM search methods. Left:Best-of-N samples N full answers and then selects the best\\nanswer according to the PRM final score. Center: Beam search samples N candidates at each step, and selects the top M\\naccording to the PRM to continue the search from. Right:lookahead-search extends each step in beam-search to utilize a k-step\\nlookahead while assessing which steps to retain and continue the search from. Thus lookahead-search needs more compute.\\nLookahead search. Lookahead search modifies how beam search evaluates individual steps. It uses\\nlookahead rollouts to improve the accuracy of the PRMâ€™s value estimation in each step of the search\\nprocess. Specifically, at each step in the beam search, rather than using the PRM score at the current step\\nto select the top candidates, lookahead search performs a simulation, rolling out up to ğ‘˜steps further\\nwhile stopping early if the end of solution is reached. To minimize variance in the simulation rollout,\\nwe perform rollouts using temperature 0. The PRMâ€™s prediction at the end of this rollout is then used\\nto score the current step in the beam search. That is, in other words, we can view beam search as a\\nspecial case of lookahead search with ğ‘˜=0. Given an accurate PRM, increasing ğ‘˜should improve the\\naccuracy of the per-step value estimates at the cost of additional compute. Also note that this version of\\nlookahead search is a special case of MCTS [ 38], wherein the stochastic elements of MCTS, designed\\nto facilitate exploration, are removed since the PRM is already trained and is frozen. These stochastic\\nelements are largely useful for learning the value function (which weâ€™ve already learned with our PRM),\\nbut less useful at test-time when we want to exploit rather than explore. Therefore, lookahead search is\\nlargely representative of how MCTS-style methods would be applied at test-time.\\n5.3. Analysis Results: Test-Time Scaling for Search with Verifiers\\nWe now present our results comparing various search algorithms and identify a prompt difficulty depen-\\ndent compute-optimal scaling strategy for search methods.\\nComparing search algorithms. We first conduct a sweep over various search settings. In addition to the\\nstandard best-of-N approach, we sweep over the two main parameters that distinguish different tree-\\nsearch methods: beam-width ğ‘€and number of lookahead steps ğ‘˜. While we are not able to extensively\\nsweep every single configuration, we sweep over the following settings with a maximum budget of 256:\\n1) Beam search with the beam width set toâˆš\\nğ‘, whereğ‘is the generation budget.\\n2) Beam search with a fixed beam width of 4.\\n8\\n21\\n23\\n25\\n27\\n29\\nGeneration Budget10152025303540MATH Test Accuracy (%)\\nComparing PRM Search Methods\\nBest-of-N Weighted\\nMajority\\nBeam; M := sqrt(N)\\nBeam; M := 4\\n1 Step Lookahead; M := sqrt(N)\\n3 Step Lookahead; M := sqrt(N)\\n3 Step Lookahead; M := 4\\n1 2 3 4 5\\nTest Questions Binned by Increasing Difficulty Level020406080MATH Test Accuracy (%)Comparing Beam Search and Best-of-N by Difficulty Level\\nBeam Search\\nBest-of-N Weighted\\nMajorityFigure 3 âˆ£Left:Comparing different methods for conducting search against PRM verifiers . We see that at low generation\\nbudgets, beam search performs best, but as we scale the budget further the improvements diminish, falling below the best-of-N\\nbaseline. Lookahead-search generally underperforms other methods at the same generation budget. Right:Comparing beam\\nsearch and best-of-N binned by difficulty level . The four bars in each difficulty bin correspond to increasing test-time compute\\nbudgets (4, 16, 64, and 256 generations). On the easier problems (bins 1 and 2), beam search shows signs of over-optimization\\nwith higher budgets, whereas best-of-N does not. On the medium difficulty problems (bins 3 and 4), we see beam search\\ndemonstrating consistent improvements over best-of-N.\\n3) Lookahead search with ğ‘˜=3applied to both beam-search settings 1) and 2).\\n4) Lookahead search with ğ‘˜=1applied to beam-search setting 1).\\nTo compare search methods as a function of generation budget fairly, we build a protocol for estimating\\nthe cost of each method. We consider a generation to be a sampled answer from the base LLM. For beam\\nsearch and best-of-N the generation budget corresponds to the number of beams and ğ‘respectively.\\nLookaheadsearch,however,utilizesadditionalcompute: ateachstepofthesearch,wesample ğ‘˜additional\\nsteps ahead. Therefore, we define the cost of lookahead-search to be ğ‘Ã—(ğ‘˜+1)samples.\\nResults. As shown in Figure 3 (left), with smaller generation budgets, beam search significantly out-\\nperforms best-of-N. However, as the budget is scaled up, these improvements greatly diminish, with\\nbeam search often underperforming the best-of-N baseline. We also see that, lookahead-search generally\\nunderperforms other methods at the same generation budget, likely due to the additional computation\\ninducted by simulating the lookahead rollouts. The diminishing returns from search are likely due to\\nexploitation of the PRMâ€™s predictions. For example, we see some instances (such as in Figure 29), where\\nsearch causes the model to generate low-information repetitive steps at the end of a solution. In other\\ncases, we find that over-optimizing search can result in overly short solutions consisting of just 1-2 steps.\\nThis explains why the most powerful search method (i.e., lookahead search) underperforms the most.\\nWe include several of these examples found by search in Appendix M.\\nWhich problems does search improve? To understand how to compute-optimally scale search methods,\\nwe now conduct a difficulty bin analysis. Specifically, we compare beam-search ( ğ‘€=4) against best-of-N.\\nIn Figure 3 (right) we see that while in aggregate, beam search and best-of-N perform similarly with a\\nhigh generation budget, evaluating their efficacy over difficulty bins reveals very different trends. On the\\neasy questions (levels 1 and 2), the stronger optimizer of the two approaches, beam search, degrades\\nperformance as the generation budget increases, suggesting signs of exploitation of the PRM signal. In\\ncontrast, on the harder questions (levels 3 and 4), beam search consistently outperforms best-of-N. On\\nthe most difficult questions (level 5), no method makes much meaningful progress.\\n9\\nThese findings match intuition: we might expect that on the easy questions, the verifier will make mostly\\ncorrect assessments of correctness. Therefore, by applying further optimization via beam search, we only\\nfurther amplify any spurious features learned by the verifier, causing performance degredation. On the\\nmore difficult questions, the base model is much less likely to sample the correct answer in the first place,\\nso search can serve to help guide the model towards producing the correct answer more often.\\n21\\n23\\n25\\n27\\n29\\nGeneration Budget10152025303540MATH Test Accuracy (%)\\nCompute Optimal Search\\nMajority\\nORM Best-of-N Weighted\\nPRM Best-of-N Weighted\\nPRM Compute Optimal Oracle\\nPRM Compute Optimal Predicted\\nFigure 4 âˆ£Comparing compute-optimal test-time compute\\nallocation against baselines with PRM search. By scaling test\\ntime compute per the notion of question difficulty, we find that\\nwe can nearly outperform PRM best-of-N using up to 4Ã—less\\ntest-time compute (e.g. 16 verses 64 generations). â€œ Compute-\\noptimal oracle â€ refers to using oracle difficulty bins derived\\nfrom the groundtruth correctness information, and â€œ compute-\\noptimal predicted â€ refers to using the PRMâ€™s predictions to\\ngenerate difficulty bins. Observe that the curves with either type\\nof difficulty bins largely overlap with each other.Compute-optimal search. Given the above re-\\nsults, it is clear that question difficulty can be a\\nuseful statistic to predict the optimal search strat-\\negy to use at a given compute budget. Addition-\\nally, the best choice of search strategy can vary\\ndrastically as a function of this difficulty statis-\\ntic. We therefore visualize the â€œcompute-optimalâ€\\nscaling trend, as represented by the best perform-\\ning search strategy at each difficulty level in Fig-\\nure 4. We see that in the low generation budget\\nregime, using both the oracle and predicted diffi-\\nculty,compute-optimal scaling can nearly out-\\nperform best-of-N using up to 4xless test-time\\ncompute (e.g. 16 verses 64 generations). While\\nin the higher budget regime, some of these ben-\\nefits diminish with the use of predicted difficulty,\\nwith oracle bins we still see continued improve-\\nments from optimally scaling test-time compute.\\nThis result demonstrates the performance gains\\nthat could be obtained by adaptively allocating\\ntest-time compute during search.\\nTakeaways for compute-optimal scaling of verifiers\\nWe find that the efficacy of any given verifier search method depends critically on both the compute\\nbudget and the question at hand. Specifically, beam-search is more effective on harder questions\\nandatlowercomputebudgets, whereasbest-of-Nismoreeffectiveoneasierquestionsandathigher\\nbudgets. Moreover, by selecting the best search setting for a given question difficulty and test-time\\ncompute budget, we can nearly outperform best-of-N using up to 4xless test-time compute.\\n6. Refining the Proposal Distribution\\nSo far, we studied the test-time compute scaling properties of search against verifiers. Now we turn\\nto studying the scaling properties of modifying the proposal distribution (Section 2). Concretely, we\\nenable the model to revise their own answers iteratively, allowing the model to dynamically improve itâ€™s\\nown distribution at test time. Simply prompting existing LLMs to correct their own mistakes tends to be\\nlargely ineffective for obtaining performance improvements on reasoning problems [ 15]. Therefore, we\\nbuild on the recipe prescribed by Qu et al. [28], incorporate modifications for our setting, and finetune\\nlanguage models to iteratively revise their own answers. We first describe how we train and use models\\nthat refine their own proposal distribution by sequentially conditioning on their own previous attempts\\nat the question. We then analyze the inference-time scaling properties of revision models.\\n10\\nQuestion          \\n    \\n     \\n Parallel Best-of-N Sequential Revisions \\nCombining Sequential / Parallel Verifier \\nselects \\nthe best \\nanswer Verifier selects \\nthe best answer \\nVerifier selects the best \\nanswer within each chain Verifier \\nselects the \\nbest answer \\nacross chains Question \\nQuestion Q:  If 4 daps = 7 \\nyaps, and 5 \\nyaps = 3 baps, \\nhow many daps \\nequal 42 baps? LMA: So 7/4 yap/dap â€¦ \\nA: We have 4 dapâ€¦ \\nA: If 7/4 yaps/dap ... \\nâ€¦\\nA: If 7/4 ... A: So â€¦ A: We â€¦ Using Revision Model + Verifier at \\nInference Time \\n                 \\n                      =  Apply Verifier           =  Selected by verifier              =  Rejected by verifier Key: \\nLMQ:  If 4 daps = 7 \\nyaps, and 5 \\nyaps = 3 baps, \\nhow many daps \\nequal 42 baps? Parallel Sampling \\nSequential Revisions \\nLM proposes a sequence of revisions, each \\nconditioned on previous revisions LM proposes answers \\nindependently, in \\nparallel Figure 5 âˆ£Parallel sampling (e.g., Best-of-N) verses sequential revisions. Left:Parallel sampling generates N answers\\nindependently in parallel, whereas sequential revisions generates each one in sequence conditioned on previous attempts. Right:\\nIn both the sequential and parallel cases, we can use the verifier to determine the best-of-N answers (e.g. by applying best-of-N\\nweighted). We can also allocate some of our budget to parallel and some to sequential, effectively enabling a combination of the\\ntwo sampling strategies. In this case, we use the verifier to first select the best answer within each sequential chain and then\\nselect the best answer accross chains.\\n6.1. Setup: Training and Using Revision Models\\nOur procedure for finetuning revision models is similar to [ 28], though we introduce some crucial\\ndifferences. For finetuning, we need trajectories consisting of a sequence of incorrect answers followed\\nby a correct answer, that we can then run SFT on. Ideally, we want the correct answer to be correlated\\nwith the incorrect answers provided in context, so as to effectively teach the model to implicitly identify\\nmistakes in examples provided in-context, followed by correcting those mistakes by making edits as\\nopposed to ignoring the in-context examples altogether, and trying again from scratch.\\nGenerating revision data. The on-policy approach of Qu et al. [28]for obtaining several multi-turn\\nrollouts was shown to be effective, but it was not entirely feasible in our infrastructure due to compute\\ncosts associated with running multi-turn rollouts. Therefore, we sampled 64 responses in parallel at\\na higher temperature and post-hoc constructed multi-turn rollouts from these independent samples.\\nSpecifically, following the recipe of [ 1], we pair up each correct answer with a sequence of incorrect\\nanswers from this set as context to construct multi-turn finetuning data. We include up to four incorrect\\nanswers in context, where the specific number of solutions in context is sampled randomly from a uniform\\ndistribution over categories 0 to 4. We use a character edit distance metric to prioritize selecting incorrect\\nanswers which are correlated with the final correct answer (see Appendix H). Note that token edit\\ndistance is not a perfect measure of correlation, but we found this heuristic to be sufficient to correlate\\nincorrect in-context answers with correct target answers to facilitate training a meaningful revision\\nmodel, as opposed to randomly pairing incorrect and correct responses with uncorrelated responses.\\nUsing revisions at inference-time. Given a finetuned revision model, we can then sample a sequence of\\nrevisions from the model at test-time. While our revision model is only trained with up to four previous\\nanswers in-context, we can sample longer chains by truncating the context to the most recent four revised\\nresponses. In Figure 6 (left), we see that as we sample longer chains from the revision model, the modelâ€™s\\npass@1 at each step gradually improves, demonstrating that we are able to effectively teach the model to\\nlearn from mistakes made by previous answers in context.\\n11\\n0 10 20 30 40 50 60\\nNumber of Generations17181920212223242526MATH Test Accuracy (%)\\nRevision Model Pass@1 At Each Step\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\nNumber of Generations2025303540MATH Test Accuracy (%)\\nRevision Model Parallel Verses Sequential\\nSequential Best-of-N Weighted\\nParallel Best-of-N Weighted\\nSequential Majority\\nParallel MajorityFigure 6 âˆ£Left:Our revision modelâ€™s pass@1 at each revision step. Pass@1 gradually improves after each revision step,\\neven improving beyond the 4 revision steps that it was trained for. We estimate pass@1 at each step by by averaging over the\\nperformance of 4 revision trajectories of length 64 for each question in the test-set. Right:Sequential vs parallel sampling\\nfrom the revision model. Comparing performance when generating N initial answers in parallel from our revision model, verses\\ngenerating N revisions sequentially, with the model. When using both the verifier and majority voting to select the answer, we\\nsee that generating answers sequentially with the revision model narrowly outperforms generating them in parallel.\\nThat said, there is a distribution shift at inference time: the model was trained on only sequences with\\nincorrect answers in context, but at test-time the model may sample correct answers that are included\\nin the context. In this case, it may incidentally turn the correct answer into an incorrect answer in the\\nnext revision step. We find that indeed, similar to Qu et al. [28], around 38% of correct answers get\\nconverted back to incorrect ones with our revision model using a naÃ¯ve approach. Therefore, we employ\\na mechanism based on sequential majority voting or verifier-based selection to select the most correct\\nanswer from the sequence of revisions made by the model (see Figure 5) to produce the best answer.\\nComparisons. To test the efficacy of modifying the proposal distribution via revisions, we setup an even\\ncomparison between the performance of sampling N revisions in sequence and sampling N attempts at\\na question in parallel. We see in Figure 6 (right), that with both the verifier-based and majority-based\\nselection mechanisms sampling solutions in sequence outperforms sampling them in parallel.\\n6.2. Analysis Results: Test-Time Scaling with Revisions\\nWe saw previously that proposing answers sequentially outperforms proposing them in parallel. However,\\nwe might expect sequential and parallel sampling to have different properties. Sampling answers in\\nparallel may act as more of a global search process, that could in principle, provide coverage over many\\ntotallydifferentapproachesforsolvingaproblem, forinstance, differentcandidatesmightutilizedifferent\\nhigh-level approaches altogether. Sequential sampling, on the other hand, may work more as a local\\nrefinement process, revising responses that are already somewhat on the right track. Due to these\\ncomplementary benefits, we should strike a balance between these two extremes by allocating some of\\nour inference-time budget to parallel sampling (e.g.âˆš\\nğ‘) and the rest to sequential revisions (e.g.âˆš\\nğ‘).\\nWe will now show the existence of a compute-optimal ratio between sequential and parallel sampling,\\nand understand their relative pros and cons based on difficulty of a given prompt.\\nTrading off sequential and parallel test-time compute. To understand how to optimally allocate\\nsequentialandparallelcompute, weperformasweepoveranumberofdifferentratios. Wesee, inFigure7\\n(left), that indeed, at a given generation budget, there exists an ideal sequential to parallel ratio, that\\n12\\n27\\n25\\n23\\n21\\n21\\n23\\n25\\n27\\nSequential/Parallel Ratio15202530354045MATH Test Accuracy (%)\\nVarying Sequential/Parallel with Verifier\\n1 2 3 4 5\\nTest Questions Binned by Increasing Difficulty Level020406080MATH Test Accuracy (%)Revisions@128, Varying the Sequential to Parallel Ratio\\n100101102\\nNumber of Generations\\n102\\n101\\n100101102\\nSequential to Parallel Ratio\\nFigure 7 âˆ£Left:Varying the ratio of the generation budget allocated sequential revisions to verses parallel samples. Each\\nline represents a fixed generation budget as the ratio is changed. We use the verifier for answer selection. We see that while\\nincreased sequential revisions tends to outperform more parallel compute, at higher generation budgets there is an ideal ratio\\nthat strikes a balance between the two extremes. Right:Varying the sequential to parallel ratio for a generation budget of\\n128 across difficulty bins. Using verifier-based selection, we see that the easier questions attain the best performance with full\\nsequential compute. On the harder questions, there is an ideal ratio of sequential to parallel test-time compute.\\nachieves the maximum accuracy. We also see in Figure 7 (right) that the ideal ratio of sequential to\\nparallel varies depending on a given questionâ€™s difficulty. In particular, easy questions benefit more from\\nsequential revisions, whereas on difficult questions it is optimal to strike a balance between sequential\\nand parallel computation. This finding supports the hypothesis that sequential revisions (i.e., varying the\\nproposal distribution) and parallel sampling (i.e., search with verifiers) are two complementary axes for\\nscaling up test-time compute, which may be more effective on a per-prompt basis. We include examples\\nof our modelâ€™s generations in Appendix L. Additional results are shown in Appendix B.\\n21\\n23\\n25\\n27\\nGeneration Budget202530354045MATH Test Accuracy (%)\\nCompute Optimal Revisions\\nMajority\\nBest-of-N Weighted\\nCompute Optimal Oracle\\nCompute Optimal Predicted\\nParallel\\nFigure 8 âˆ£Comparing compute-optimal test-time compute\\nallocation against the parallel compute baseline with our re-\\nvision model . By optimally scaling test-time compute according\\nto question difficulty, we find that we can outperform best-of-N\\nusing up to 4xless test-time compute (e.g. 64 samples verses\\n256). â€œCompute-optimal oracle â€ refers to using the oracle\\ndifficulty bins derived from the ground truth correctness infor-\\nmation, and â€œ compute optimal predicted â€ refers to using the\\nPRMâ€™s predictions to produce model-predicted difficulty bins.Compute-optimal revisions. Given that the effi-\\ncacy of sequential and parallel sampling depends\\non question difficulty, we can select the ideal ratio\\nofsequentialtoparallelcomputeperdifficultybin.\\nIn Figure 8, we plot results using this compute-\\noptimalscalingstrategywhenemployingbothour\\noracle and predicted notions of difficulty. In both\\ncases, weâ€™re able to substantially improve test-\\ntime compute scaling by improving the proposal\\ndistribution via revisions. In particular, we see\\nthat at higher generation budgets, parallel sam-\\npling seems to plateau, whereas compute-optimal\\nscaling demonstrates continued improvements.\\nFor both oracle and predicted difficulty bins, we\\nsee thatcompute-optimal scaling can outper-\\nform best-of-N using up to 4xless test-time\\ncompute (e.g. 64 samples verses 256). Overall,\\nthese results demonstrate the potential for im-\\nproved test-time compute scaling by adjusting the\\nproposal distribution on a per-prompt basis.\\n13\\nTakeaways for compute-optimal scaling by refining the proposal distribution with revisions\\nWe find that there exists a tradeoff between sequential (e.g. revisions) and parallel (e.g. standard\\nbest-of-N) test-time computation, and the ideal ratio of sequential to parallel test-time compute\\ndepends critially on both the compute budget and the specific question at hand. Specifically,\\neasier questions benefit from purely sequential test-time compute, whereas harder questions often\\nperform best with some ideal ratio of sequential to parallel compute. Moreover, by optimally\\nselecting the best setting for a given question difficulty and test-time compute budget, we can\\noutperform the parallel best-of-N baseline using up to 4xless test-time compute.\\n7. Putting it Together: Exchanging Pretraining and Test-Time Compute\\nSo far, we saw that utilizing additional test-time computation can enable us to represent more complex\\ndistributions than the one predicted by the base LLM itself, thereby improving performance. We now\\nposit that this increased flexibility of representing distributions means that we can expect additional\\ntest-time compute to make up for the lack of a higher-capacity model or training for more FLOPs during\\npre-training. In this section, we study to what extent this is possible. We pose the following question:\\nQuestion: Exchanging pretraining and test-time compute\\nSuppose a model was pre-trained with ğ‘‹FLOPs. Assume that we plan to run ğ‘ŒFLOPs of inference\\nwith this model. If we want to improve performance by increasing the total FLOPs budget by a\\nfactor ofğ‘€(i.e.,ğ‘€(ğ‘‹+ğ‘Œ)total FLOPs across both pretraining and inference), should we spend\\nour FLOPs on increased pretraining compute or on additional test-time compute?\\nIncreasing pretraining FLOPS introduces the additional design decision of whether to allocate compute to\\ntraining with more data or more parameters [ 14]. We focus on the setting in which model parameters are\\nscaled up and training data amount is fixed, matching the approach taken with the open-source LLaMA\\nseries of models [ 41]. We choose this setting as it is representative of a canonical approach to scaling\\npretraining compute and leave the analysis of compute-optimal scaling of pretraining compute [ 29]\\nwhere the data and parameters are both scaled equally to future work.\\nDefining an exchange rate between FLOPs. We now describe how we define the exchange rate between\\npretraining and inference FLOPs. To determine pretraining FLOPs, use use the common approximation\\nğ‘‹=6ğ‘ğ·pretrain[14], and for inference FLOPs, we use ğ‘Œ=2ğ‘ğ·inference[29]. Hereğ‘represents model\\nparameters, ğ·pretrainis the number of tokens used for pretraining, and ğ·inferencethe total number of\\ntokens generated at inference time. With these approximations, we can see that, if we multiply the model\\nparameters by a factor of ğ‘€, then both the pretraining and inference FLOPs (due to the cost of greedy\\ndecoding with the larger model), increase by a factor of ğ‘€(givingğ‘€(ğ‘‹+ğ‘Œ)total FLOPs).\\nTo match the FLOPs from scaling up model paramters using test-time compute with the smaller model\\nwe can multiply the smaller modelâ€™s inference compute by a factor of ğ‘€+3(ğ·pretrain\\nğ·inference)(ğ‘€âˆ’1). Notably,\\nthe amount of inference compute we can utilize to match the FLOPs for the larger model depends on\\nthe ratioğ·pretrain\\nğ·inference. We refer to the inverse of this ratio as ğ‘…(e.g.ğ·inference\\nğ·pretrain). Depending on the specific\\nproduction setting or use-case, we should expect very different values of ğ‘…. In particular, in many large\\nscale production settings, we may expect significantly more inference tokens than pretraining tokens, in\\nwhich case we would have ğ‘…>>1. On the other hand, in many contemporary self-improvement setups,\\nthat would use test-time compute to improve the model, we would likely generate significantly fewer\\n14\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\nProportional to Inference FLOPs20406080100MATH Difficulty Level Accuracy (%)\\nRevisions\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\nProportional to Inference FLOPs20406080MATH Difficulty Level Accuracy (%)\\nPRM Search\\n1\\n2\\n3\\n4\\n5\\nDifficulty Level\\nPretraining Compute Test-time Compute R >> 1 R ~= 1 R << 1Comparing Test-time and Pretraining ComputeFigure 9 âˆ£Tradeoff between pretraining and test-time compute in a FLOPs-matched evaluation. Each line represents the\\nperformance of scaling test-time compute with our compute-optimal policy in each oracle difficulty bin. We plot the results for\\nrevisions on the left and search on the right. The stars represent the greedy pass@1 performance of a base model pretrained\\nwithâˆ¼14times more parameters. We plot test-time compute budget on the x-axis, and place the stars at three different\\nlocations along the x-axis, each corresponding to the FLOPs equivalent point of comparison between scaling parameters and\\nscaling test-time compute for three different inference compute loads (e.g. ğ‘…=ğ·inference\\nğ·pretrain). If the star is below the line, this\\nimplies that it is more effective to use test-time compute than to scale model parameters, and if the star is above the line this\\nimplies that scaling parameters is more effective. We see that on the easy questions or in settings with a lower inference load\\n(e.g.ğ‘…<<1), test-time compute can generally outperform scaling model parameters. However, on the harder questions or in\\nsettings with a higher inference load (e.g. ğ‘…>>1), pretraining is a more effective way to improve performance.\\ninference tokens than pretraining tokens, giving ğ‘…<<1. Therefore, since the scale of test-time compute\\nwe can apply is dependent on this ratio, we expect differing conclusions depending on the specific setting.\\nIn Figure 9, we use this approach to exchanging test-time and pretraining compute to compare our\\ncompute-optimalscalingagainstscalingupmodelparametersbyafactorof âˆ¼14. Weconductcomparisons\\nfor 3 different values of R: 0.16 ( ğ‘…<<1), 0.79 (ğ‘…âˆ¼1), and 22 ( ğ‘…>>1), with each ratio corresponding\\nto an inference budget. Observe that if we only expect to see very difficult questions (e.g. difficulty\\nbins 4/5) or have a larger ğ·inference(corresponding to a larger ğ‘…value), then it is often more effective to\\nallocate our budget towards pretraining (e.g. the star is above the line). If instead, we expect mostly\\neasy or intermediate difficulty questions (e.g. bins 1/2/3 and sometimes 4) or have lower inference\\nrequirements (as is the case in self-improvement pipelines), then utilizing test-time compute is better.\\nTakeaways for exchanging pretraining and test-time compute\\nTest-time and pretraining compute are not 1-to-1 â€œexchangeableâ€. On easy and medium questions,\\nwhich are within a modelâ€™s capabilities, or in settings with small inference requirement, test-time\\ncompute can easily cover up for additional pretraining. However, on challenging questions which\\nare outside a given base modelâ€™s capabilities or under higher inference requirement, pretraining is\\nlikely more effective for improving performance.\\n8. Discussion and Future Work\\nIn this work, we conducted a thorough analysis of the efficacy of different techniques that aim to either\\nimprove search against a verifier or to refine an LLMâ€™s proposal distribution, for scaling test-time compute\\n15\\nfor math reasoning. In general, we found that the efficacy of a given approach heavily correlates with\\nthe difficulty of the problem from the perspective of the base LLMâ€™s capabilities. This motivated us to\\nintroduce the notion of â€œcompute-optimalâ€ scaling of test-time computation, which prescribes a adaptive,\\nprompt-dependent strategy to improve performance under a given test-time compute budget. By applying\\nsuch a compute-optimal scaling strategy, we find that can improve the efficiency of test-time compute\\nscaling by a factor of 2âˆ’4Ã—. When comparing benefits obtained from additional test-time compute\\nagainst benefits from additional pre-training compute in a FLOPs-matched setting, we show for the first\\ntime that using test-time computation with seemingly simple methods (i.e., revisions and search) can\\nalready scale well on certain types of prompts, providing gains over spending those FLOPs in pretraining.\\nThat said, there are also limitations associated with our study that future work can aim to address.\\nFurther improving test-time compute scaling. In this work we focused on improving the test-time\\ncompute scaling of two primary mechanisms: the verifier and the proposal distribution (via revisions).\\nWhile we combined verifiers with revisions in Section 6, we did not experiment with PRM tree-search\\ntechniques in combination with revisions. Neither did we study other techniques such as critique and\\nrevise [23]. Future work should investigate how test-time compute scaling can be further improved by\\ncombining a variety of these approaches. Additionally, we found that across the board these schemes\\nprovided small gains on hard problems; future work should work to develop new ways of using test-time\\ncompute which can circumvent this limitation.\\nAssessing question difficulty quickly. We used a notion question difficulty as a simple sufficient statistic\\nfor approximating the compute-optimal test-time scaling strategy. While this scheme was effective,\\nestimating our notion of difficulty requires applying a non-trivial amount of test-time compute itself.\\nFuture work should consider alternative ways of more efficiently estimating question difficulty (e.g., by\\npretraining or finetuning models to directly predict difficulty of a question) or dynamically switching\\nbetween assessing difficulty and attempting to solve a question.\\nInterleaving test-time and training-time compute. We focused purely on test-time compute scaling\\nin this work and the degree to which test-time compute can be traded off for additional pretraining.\\nHowever, in the future, we envision that the outputs of applying additional test-time compute can be\\ndistilled back into the base LLM, enabling an iterative self-improvement loop that operates on open-ended\\nnatural language. To this end, future work should extend our findings and study how the outputs of\\napplying test-time compute can be used to improve the base LLM itself.\\nAcknowledgements\\nWe thank Yi Su, Rishabh Agarwal, Yinlam Chow, Aleksandra Faust, Vincent Zhuang, George Tucker,\\nHao Liu, Jiayi Pan, Ethan Dyer, Behnam Neyshabur, Xavier Garcia, Yamini Bansal, Lampros Lamprou,\\nYuxiao Qu, and Amrith Setlur for their feedback on an earlier version of the paper and discussions. We\\nattribute and thank Rishabh Agarwal, Vincent Zhuang, Yi Su, and Avi Singh for ideas and discussions,\\nand experiments that concretely demonstrated the promise of pairwise sample generation for training\\nrevision models, and edit distance based sampling in [ 1]. We thank Slav Petrov for leadership support.\\nReferences\\n[1] Training revision models with synthetic data. Coming soon, 2024.\\n16\\n[2]C. Andrieu, N. De Freitas, A. Doucet, and M. I. Jordan. An introduction to mcmc for machine\\nlearning. 2003.\\n[3]R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\\nZ. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira,\\nM. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin,\\nP. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A.\\nChoquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. DÃ­az, N. Du,\\nE. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari,\\nS. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah,\\nM. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li,\\nW. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra,\\nM. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov,\\nR. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby,\\nA. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang,\\nP. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng,\\nC. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report, 2023.\\n[4]Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,\\nC. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-\\nJohnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt,\\nM. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer,\\nS. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan,\\nT.Hume, S.R.Bowman, Z.Hatfield-Dodds, B.Mann, D.Amodei, N.Joseph, S.McCandlish, T.Brown,\\nand J. Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022.\\n[5]C. Blakeney, M. Paul, B. W. Larsen, S. Owen, and J. Frankle. Does your data spark joy? performance\\ngains from domain upsampling at the end of training, 2024. URL https://arxiv.org/abs/\\n2406.03476 .\\n[6]G. Chen, M. Liao, C. Li, and K. Fan. Alphamath almost zero: process supervision without process,\\n2024.\\n[7]K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,\\nR. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021.\\n[8]Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Improving factuality and reasoning in\\nlanguage models through multiagent debate, 2023.\\n[9]J. S. B. T. Evans. Heuristic and analytic processes in reasoning. British Journal of Psychology , 75(4):\\n451â€“468, 1984.\\n[10]X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search\\ncan guide large language model decoding and training, 2024.\\n[11]L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-aided\\nlanguage models, 2023. URL https://arxiv.org/abs/2211.10435 .\\n[12]S. Goyal, Z. Ji, A. S. Rawat, A. K. Menon, S. Kumar, and V. Nagarajan. Think before you speak: Train-\\ning language models with pause tokens, 2024. URL https://arxiv.org/abs/2310.02226 .\\n17\\n[13]D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.\\nMeasuring mathematical problem solving with the math dataset, 2021.\\n[14]J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training\\ncompute-optimal large language models, 2022.\\n[15]J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models\\ncannot self-correct reasoning yet, 2023.\\n[16]A. L. Jones. Scaling scaling laws with board games, 2021. URL https://arxiv.org/abs/2104.\\n03113.\\n[17]D. Kahneman. Maps of bounded rationality: Psychology for behavioral economics. The American\\nEconomic Review , 93(5):1449â€“1475, 2003.\\n[18]D. Kahneman. Thinking, fast and slow . Farrar, Straus and Giroux, New York, first paperback edition\\nedition, 2013.\\n[19]L. Kocsis and C. Szepesvâ€™ari. Bandit based monte-carlo planning. In European conference on machine\\nlearning , pages 282â€“293. Springer, 2006.\\n[20]A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil,\\nI. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative\\nreasoning problems with language models, 2022.\\n[21]Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen. Making large language models\\nbetter reasoners with step-aware verifier, 2023.\\n[22]H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever,\\nand K. Cobbe. Letâ€™s verify step by step, 2023.\\n[23]A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\\nY. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-\\nrefine: Iterative refinement with self-feedback, 2023.\\n[24]N. McAleese, R. Pokorny, J. F. CerÃ³n Uribe, E. Nitishinskaya, M. TrÄ™bacz, and J. Leike. Llm critics\\nhelp catch llm bugs. OpenAI, 2024.\\n[25] OpenAI. Gpt-4 technical report, 2024.\\n[26]Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, L. Hong,\\nR. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun. Toolllm: Facilitating large language\\nmodels to master 16000+ real-world apis, 2023. URL https://arxiv.org/abs/2307.16789 .\\n[27]C. Qu, S. Dai, X. Wei, H. Cai, S. Wang, D. Yin, J. Xu, and J.-R. Wen. Tool learning with large\\nlanguage models: A survey, 2024. URL https://arxiv.org/abs/2405.17935 .\\n[28]Y. Qu, T. Zhang, N. Garg, and A. Kumar. Recursive introspection: Teaching foundation models how\\nto self-improve. 2024.\\n18\\n[29]N. Sardana and J. Frankle. Beyond chinchilla-optimal: Accounting for inference in language model\\nscaling laws, 2023.\\n[30]W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for\\nassisting human evaluators, 2022.\\n[31]A. Setlur, S. Garg, X. Geng, N. Garg, V. Smith, and A. Kumar. Rl on incorrect synthetic data scales\\nthe efficiency of llm math reasoning by eight-fold. arXiv preprint arXiv:2406.14532 , 2024.\\n[32]Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo.\\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.\\n[33]A. Sharma, S. Keh, E. Mitchell, C. Finn, K. Arora, and T. Kollar. A critical evaluation of ai feedback\\nfor aligning large language models, 2024. URL https://arxiv.org/abs/2402.12366 .\\n[34]N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language\\nagents with verbal reinforcement learning, 2023.\\n[35]A. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, X. Garcia, P. J. Liu, J. Harrison, J. Lee, K. Xu,\\nA. Parisi, A. Kumar, A. Alemi, A. Rizkowsky, A. Nova, B. Adlam, B. Bohnet, G. Elsayed, H. Sedghi,\\nI. Mordatch, I. Simpson, I. Gur, J. Snoek, J. Pennington, J. Hron, K. Kenealy, K. Swersky, K. Mahajan,\\nL. Culp, L. Xiao, M. L. Bileschi, N. Constant, R. Novak, R. Liu, T. Warkentin, Y. Qian, Y. Bansal,\\nE. Dyer, B. Neyshabur, J. Sohl-Dickstein, and N. Fiedel. Beyond human data: Scaling self-training\\nfor problem-solving with language models, 2024.\\n[36]C. Snell, E. Wallace, D. Klein, and S. Levine. Predicting emergent capabilities by finetuning.\\nConference on Language Modeling 2024 , 2024.\\n[37]K. Stechly, M. Marquez, and S. Kambhampati. Gpt-4 doesnâ€™t know itâ€™s wrong: An analysis of iterative\\nprompting for reasoning problems, 2023.\\n[38] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction . Second edition, 2018.\\n[39]G. Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,\\n2024.\\n[40]Y. Tian, B. Peng, L. Song, L. Jin, D. Yu, H. Mi, and D. Yu. Toward self-improvement of llms via\\nimagination, searching, and criticizing, 2024.\\n[41]H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,P.Bhargava,\\nS. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu,\\nW. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan,\\nM. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril,\\nJ. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton,\\nJ. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan,\\nB. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur,\\nS. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and\\nfine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288 .\\n[42]J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins.\\nSolving math word problems with process- and outcome-based feedback, 2022.\\n19\\n[43]K. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by\\nself-critiquing their own plans?, 2023.\\n[44]P. Villalobos and D. Atkinson. Trading off compute in training and inference, 2023. URL https:\\n//epochai.org/blog/trading-off-compute-in-training-and-inference . Accessed:\\n2024-07-03.\\n[45]P. Wang, L. Li, Z. Shao, R. X. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify\\nand reinforce llms step-by-step without human annotations, 2023.\\n[46]R. Wang, E. Zelikman, G. Poesia, Y. Pu, N. Haber, and N. D. Goodman. Hypothesis search: Inductive\\nreasoning with language models, 2024. URL https://arxiv.org/abs/2309.05660 .\\n[47]J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-\\nthought prompting elicits reasoning in large language models, 2023.\\n[48]S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:\\nDeliberate problem solving with large language models, 2023.\\n[49]Z. Yuan, H. Yuan, C. Li, G. Dong, K. Lu, C. Tan, C. Zhou, and J. Zhou. Scaling relationship on\\nlearning mathematical reasoning with large language models, 2023.\\n[50]E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning,\\n2022.\\n[51]E. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. D. Goodman. Quiet-star: Language\\nmodels can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/\\n2403.09629 .\\n20\\nAppendices\\nA. Related Work\\nLanguage model reasoning. Language model performance on challenging mathematical reasoning tasks\\nhas rapidly improved in recent years [ 20,22,25,32,39]. These improvements can be attributed to four\\nprimary factors: 1)running continued pretraining on large corpora of math focused data [ 20,22,32,39];\\n2)improving the LLM proposal distribution by either applying targeted optimization on specific reasoning\\ntasks by finetuning with RL [ 32,35,49,50] enabling models to critique and revise their answers\\niteratively [ 4,8,23,30];3)enabling LLMs to benefit from additional test-time computation by finetuning\\nverifiers [ 6,7,10,22,40,42,45,48]. Our work builds on these second and third lines of research by\\nanalyzing the extent to which test-time compute scaling can be improved by 1) refining an LLMâ€™s proposal\\ndistribution and 2) conducting search against verifiers.\\nAnalyzing test-time compute scaling. The tradeoff between train-time and test-time compute using\\nMonte-Carlo tree search applied to the board game Hex was previously studied by Jones [16]. We instead\\nfocus our analysis on full-scale language model math reasoning problems. A survey work by Villalobos\\nand Atkinson [44]analyzed the tradeoff between training and inference across a number of domains.\\nHowever, much of their language-model analysis focused on test-time compute scaling in settings where\\nthe ground-truth answer is known. In contrast, our analysis focuses on the setting when the ground-truth\\nanswer is not known. Additionally, a number of works in the RL literature have proposed methods, such\\nas MCTS [ 19], which aim to navigate the tradeoff between test-time and training-time compute so as\\nto enable a form of iterative self-play. The findings in our work can be used to help develop similar\\nalgorithms that can operate on open-ended natural language.\\nAugmentingLLMswithtest-timecompute. Beyondverifiersandrevisions, anumberofadditionalworks\\nhaveproposedalternativemethodsforenablingLMstousetest-timecomputeforreasoning. Namely,Wang\\net al.[46]conducts a hierarchical hypothesis search to enable inductive reasoning capabilities. A number\\nof related works have proposed augmenting language models with tools at test-time, which can greatly\\nimprove their performance on downstream tasks [ 11,26,27]. Finally, several works have proposed\\nmethods for learning thought tokens in an unsupervised manner [ 12,51], enabling models to more\\neffectively utilize the additional test-time compute that comes with sampling longer sequences. While we\\nfocus our analysis on two primary mechanisms by which test-time compute can be scaled in this work (e.g.\\nverifiers and revisions), many of the methods by which we conduct our analysis (e.g. compute optimal\\nscaling according to question difficulty) could, in principle, also be applied to any of these other methods\\nof scaling test-time compute, and we believe that this is an interesting direction for future research.\\nB. Additional Revision Results\\nWe plot additional results for majority selection using out PaLM 2-S* revision model in Figure 10. With\\nmajority selection, we see largely similar trends to those found in Figure 7 for verifier selection.\\nC. Unsupervised Difficulty Bins\\nWe compute difficulty bins without oracle ground-truth correctness information by averaging the PRM\\nfinal-answer score over 2048 samples on each question, so as to obtain a value estimate corresponding to\\n21\\n27\\n25\\n23\\n21\\n21\\n23\\n25\\n27\\nSequential/Parallel Ratio2025303540MATH Test Accuracy (%)\\nVarying Sequential/Parallel with Majority\\n1 2 3 4 5\\nTest Questions Binned by Increasing Difficulty Level020406080MATH Test Accuracy (%)Revisions Majority@128, Varying the Sequential to Parallel Ratio\\n100101102\\nNumber of Generations\\n102\\n101\\n100101102\\nSequential to Parallel Ratio\\nFigure 10 âˆ£Varying the ratio of generation budget allocated to sequential verses parallel samples, using\\nmajority to select the answer, rather than the verifier. Left:Each line represents a fixed generation budget\\nas the ratio is changed. We see that similar to the verifier case, in the majority case, there exists an ideal\\nratio of sequential to parallel test-time compute at a given budget. Right:Analyzing performance across\\ndifficulty bins, we see that the easier questions are mostly invariant the ratio of sequential to parallel,\\nwhereas on the harder questions there is an ideal ratio of sequential to parallel test-time compute.\\nthe question. We then bin the value for each question in the test-set into five quintiles (using the same\\nprocedure as the oracle difficulty bins). We refer to this as â€œpredicted difficultyâ€ rather than â€œoracle\\ndifficultyâ€. Technically this procedure is extremely costly because it requires generating many samples.\\nWhile we do not account for this cost in our analysis, in a practical production setting, this cost would\\nbe problematic. A more efficient approach would be to finetune a model to predict correctness directly,\\ngiven the question. We do not explore this in our work, but leave such exploration of cheaper methods of\\nestimating difficulty to future work.\\nIn Figure 12 we plot PRM-search results using our difficulty bins, and in Figure 11 we plot the corre-\\nsponding revision results. We see that in both settings these predicted bins demonstrate similar trends to\\nthe oracle bins.\\nD. PRM Training Details\\nWe finetune our PRM as a binary classifier, where the model predicts a value between 0 and 1 at each\\nstep in the solution. We train the model with soft values obtained from the monte-carlo rollouts, using\\na binary cross entropy loss function (e.g. âˆ’(ğ‘¦ğ‘™ğ‘œğ‘”(Ë†ğ‘¦)+(1âˆ’ğ‘¦)ğ‘™ğ‘œğ‘”(1âˆ’Ë†ğ‘¦))whereğ‘¦corresponds to the\\nsoft ground-truth value and Ë†ğ‘¦the modelâ€™s predicted value). We finetune the model base model using the\\nAdamW optimizer, with lr 3e-5, batch size 128, dropout 0.05, and Adam betas (0.9,0.95). We conduct\\nearly stopping, selecting the checkpoint with the lowest validation loss on a random held-out validation\\nset, consisting of 10% of the questions in the original PRM800k training split.\\nWe finetune the PRM on 16 samples per question from the corresponding few-shot prompted base model.\\nAt each step, we use 16 monte-carlo rollouts, using the same base model and prompt, to estimate the\\nstep-level value. We filter out all samples which fail to output a valid, parsable final answer from the\\ntraining data, as we found these to hurt PRM performance in initial experiments.\\n22\\n1 2 3 4 5\\nTest Questions Binned with Unsupervised Difficulty Bins01020304050607080MATH Test Accuracy (%)Revisions Best-of-128 Weighted, Varying the Sequential to Parallel Ratio\\n102\\n101\\n100101102\\nSequential to Parallel Ratio\\n1 2 3 4 5\\nTest Questions Binned with Unsupervised Difficulty Bins01020304050607080MATH Test Accuracy (%)Revisions Majority@128, Varying the Sequential to Parallel Ratio\\n102\\n101\\n100101102\\nSequential to Parallel Ratio\\nFigure 11 âˆ£Using our PaLM 2-S* PRM to compute difficulty bins without ground truth correctness\\ninformation for revisions. On the left we plot verifier selection and on the right we plot majority selectionl\\nWeseelargelysimilarperformancetrendswiththesebinsaswedowiththegroundtruthonesinFigures7\\nand 10.\\n1 2 3 4 5\\nTest Questions Binned with Unsupervised Difficulty Bins01020304050607080MATH Test Accuracy (%)Comparing Beam Search and Best-of-N with Unsupervised Difficulty Bins\\nBeam Search\\nBest-of-N Weighted\\nMajority\\nFigure 12 âˆ£Using our PaLM 2-S* PRM to compute difficulty bins without ground truth correctness\\ninformation for PRM search. We see largely similar performance trends with these bins as we do with the\\nground truth ones in Figure 3.\\n23\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\nNumber of Samples10152025303540MATH Test Accuracy (%)\\nComparing PRM Aggregation Strategies\\nPRM min\\nPRM prod\\nPRM last\\nBase-LM Majority\\nORMFigure 13 âˆ£We compare different methods of aggregating per-step PRM scores to produce a final score\\nfor the full solution: â€œminâ€ refers to taking the minimum score accross all steps, â€œprodâ€ takes the product\\nof all step correctness probabilities, and â€œlastâ€ just uses the last step score. We see that last performs the\\nbest across all aggregation strategies.\\nWhen generating the samples, the base model is prompted to output answers in newline separated a\\nstep-by-step format, as done in Lightman et al. [22]. We then separate each of the answers into steps\\nusing a simple newline splitting procedure. We include details about our prompt in Appendix G.\\nE. Comparing PRM Aggregation Strategies\\nWe compare different methods of aggregating per-step PRM scores to produce a final score for the full\\nsolution. Specifically we compare: 1) taking the minimum score accross all steps as done in Lightman\\net al.[22](e.g. â€œminâ€); 2) taking the product of all step correctness probabilities (e.g. â€œprodâ€); and 3)\\ntaking just the last step prediction (e.g. â€œlastâ€). We see in Figure 13 that taking the last step outperforms\\nthe other two approaches. Prior works [ 22,45] found min to be the best aggregator. We believe that the\\ndiscrepancy is due to the fact that our verifier was trained with soft MC return labels, which surface very\\ndifferently from binary correctness labels, and therefore other aggregation strategies may not have the\\nsame effect.\\nInterestingly, when using the last step aggregation, we are effectively using the PRM like an ORM.\\nHowever, we see that the PRM outperforms the ORM, suggesting that in our case the per-step PRM\\ntraining may be largely useful as a form of representation learning, rather than purely as a tool at\\ninference time. Future work should further explore this line of reasoning.\\nF. Comparing PRM and ORM\\nWe trained a PRM and ORM model using the PaLM 2-S* base LM. We see in Figure 14, that the PRM\\noutperforms the ORM, and the gap between the gap between the PRM and ORM grows with the number\\n24\\n21\\n23\\n25\\n27\\n29\\n211\\nNumber of Samples10152025303540MATH Test Accuracy (%)\\nORM Verses PRM\\nPRM best-of-N weighted\\nBase-LM Majority\\nORM best-of-N weightedFigure 14 âˆ£We compare PRM and ORM models finetuned from PaLM 2-S* in a best-of-N evaluation. We\\nuse the PaLM 2-S* base LM to sample outputs, using a few-shot prompt. We see that the PRM greatly\\noutperforms the ORM at a larg number of samples.\\nof samples used. We use the last step prediction from the PRM to score the answers as described in\\nAppendix E.\\nG. Prompting Details\\nIn order to enable the base model to output answers in a step-by-step format to which a PRM can be\\napplied, we use a 4-shot prompt consisting of randomly selected correct answer examples from the\\nPRM800k data released by Lightman et al. [22]. Specifically we use answers from the phase 1 training\\nsplit. These answers correspond to GPT-4 generated correct answer examples, which include the correct\\nstep-by-step format. In initial experiments, we found that this prompting procedure produces similar\\nresults to the prompt used in Lewkowycz et al. [20]. We use this prompt for generating training data for\\nthe PRM and the revision model. We also use this prompt when conducting search against the PRM on\\nthe test-set. To grade the final answer predicted by this prompt, we use the grading function released\\nby Lightman et al. [22].\\nH. Revision Model Finetuning Details\\nFor fine-tuning the revision model, we follow the procedure outlined in Section 6.1. We first sample 64\\noutputs per question. We then filter out all answers which end in an invalid solution. For each correct\\nanswer, we then sample a number uniformly between 0 and 4 indicating how many incorrect answers to\\ninclude in context for training. The correct answer is used as the last answer in the trajectory (which we\\ntrain the model to produce) and the incorrect answers are included in context. If the sampled number\\nis greater than 0, we then find the closest incorrect answer according to a character-level edit distance\\nmetric to include as the last incorrect answer in the trajectory. The goal here is to select an incorrect\\n25\\nanswer which is somewhat correlated with the correct answer, to improve learning. The remaining\\nincorrect answers, we sample randomly from the set of available answers. In the case where there are\\nfewer than 4 incorrect answers sampled, we truncate the uniform distributionâ€™s max to match the number\\nof incorrect samples. We use this procedure to generate trajectories for all questions in the training data.\\nWe then finetune the base language model on the correct answer solutions in these generated trajectories.\\nWe use the AdamW optimizer with lr 1e-5, batch size 128, dropout 0.0, and Adam betas (0.9,0.95).\\nWefindthatgenerallyevaluatinglossonanevaluationsetconsistingoftrajectoriesgeneratedasdescribed\\nabove, does not provide a good signal for early stopping. Rather, we find that checkpoints much after\\nthe evaluation loss begins increasing are much more capable of revisions. This is likely because after\\nfinetuning the revision model, the evaluation set represents off-policy data, which will naturally be out-of-\\ndistribution compared to the trajectires that the model itself would generate on-policy. We therefore select\\nour revision model checkpoint slightly after the point where we observe overfitting on the validation set.\\nI. Revision Model Selection Criteria\\nAs described in Section 6.1, in order to effective use our revision model we need to deploy a criteria for\\nselecting the best answer both within a revision trajectory and between multiple parallel trajectories. We\\nuse two approaches: 1) ORM verifier; and 2) majority voting.\\nFor the ORM verifier, we train an ORM on the revision modelâ€™s outputs according to the procedure in\\nAppendix J. At inference, time we then use this verifier to select the best answer. Since we have two axes\\nacross which to aggregate (within each revision trajectories and between multiple trajectories), we deploy\\na hierarchical strategy, first selecting the best answer within each revision trajectory and then aggregating\\nthese selected answers across trajectories. To select the best answer within each trajectory, we perform\\nbest-of-N weighted aggregation and then choose the highest scoring solution with the maximum best-of-N\\nweighted answer. Then, to select the final answer across all revision chains, we perform another round\\nof best-of-N weighted selection using the best answer from each revision chain. The answer after this\\nsecond round of best-of-N weighted represents our final answer prediction.\\nFormajorityvotingwefoundhierarchicalaggregationtocreateproblemswhenthelengthofthetrajectory\\nor the number of trajectories was too small. The problem being that without enough samples, majority\\nvoting is unable to effectively select the best option. Therefore, for majority voting, we simply take all\\nanswers, across all trajectories, at once and take their majority as the final-answer. We found this to\\nproduce much smoother scaling behavior than the hierarchical approach.\\nJ. Revision Model Verifier Training\\nWe found that the PRM we finetuned on the PaLM 2-S* base model outputs was not as effective when\\napplied to the PaLM 2-S* revision modelâ€™s outputs (see Figure 15(a)), likely due to distribution shift with\\nthe revision model. We therefore, trained a separate ORM verifier to use with our PaLM 2-S* revision\\nmodel. We could have trained a PRM as well, but opted for an ORM due to the high cost of generating\\nper-step PRM labels.\\nWe modified the standard ORM slightly for the revision setting, by finetuning the ORM with previous\\nrevision in context, such that the verifier has access to the same context as the revision model, allowing\\n26\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\nNumber of Generations15202530354045MATH Test Accuracy (%)\\nRevision Model Verifier Verses Base-LM PRM\\nSequential + Revision ORM\\nSequential + Base LM PRM\\nParallel\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\nNumber of Generations2025303540MATH Test Accuracy (%)\\nRevision Model Verifier With Verse Without History\\nSequential + Verifier With History\\nSequential + Verifier Without History\\nParallelFigure 15 âˆ£Left: we compare the ORM we trained on the revision modelâ€™s outputs against the PRM we\\ntrained on the PaLM 2-S* base modelâ€™s outputs. We see that when applied to outputs from the revision\\nmodel, the ORM adapted to the revision model outperforms the PRM, likely due to distribution shift\\nwith the revision model. Right: we ablate the effect of including previous revisions in the revision model\\nverifierâ€™s context. We see that including revisions in-context helps the verifier slightly, but both settings\\nstill outperform the parallel baseline.\\nthe verifier see the revision modelâ€™s previous answer attempts when scoring the current answer. All other\\nexperiment details are identical to those used for training the PRM.\\nEmpirically, we find that including the revision history in context improves performance slightly (see\\nFigure 15(b)). Additionally, even without the revisions in context, we see that sequential revisions still\\nslightly outperforms parallel, demonstrating improvements from sequential sampling are not just due to\\nthe verifierâ€™s context.\\nK. ReSTEMRevision Model Experiments\\nWe experimented with further optimizing our PaLM 2-S* revision model by training the model with a\\nsimplified RL algorithm: ReSTEM[35]. Specifically, we generated 64 revision trajectories of maximum\\nlength 5 for each question on the MATH training set. We stopped the revision model at the first correct\\nanswer in each trajectory. Using this generated data, we then finetuned the base LM on the correct answer\\ndata. To help the model learn the task, we explicitly balanced the distribution of trajectory lengths.\\nIn Figure 16, we plot the performance of this new revision model as we vary the sequential to parallel\\nratio. We see that additional sequential revisions substantially hurts performance with this new model.\\nWe hypothesize that this degradation is due to the fact that the online data obtained from running\\nReSTEMexacerbates spurious correlations in revision data, causing the optimized model to fail to learn\\nthe revision task. We believe that using a more offline data collection strategy, as done in Qu et al. [28],\\nmay be more effective, and leave further exploration to future work.\\n27\\n25\\n23\\n21\\n21\\n23\\n25\\nSequential/Parallel Ratio202225283032353840MATH Test Accuracy (%)\\nVarying Sequential/Parallel\\n100101102\\nNumber of Generations\\nFigure 16 âˆ£Performance of our ReSTEMoptimized revision model as the sequential to parallel ratio\\nis varied. We use majority voting to select the answer. We see that this optimized revision model\\ndemonstrates substantial performance degradations with additional sequential revisions.\\nL. Revision Model Example Outputs\\nIn Figures 17, 18, 19, 20, 21, 22, and 23, we include select examples of our revision modelâ€™s outputs.\\nM. PRM Beam Search Example Outputs\\nIn Figures 24, 25, 26, 27, 28, and 29, we include select examples of PRM beam search. We include the\\nPRM score, between 0 and 1, for each step in the examples.\\n28\\nFigure 17 âˆ£Revision model example 1. The model calculates the sum at the end incorrectly on the first\\ntwo attempts, but on the third attempt it succeeds and gets the answer correct.\\n29\\nFigure 18 âˆ£Revision model example 2. On the first attempt the model takes the incorrect approach, on\\nthe second attempt it gets closer but then makes a mistake towards the end. On the final attempt it gets\\nto the correct answer.\\n30\\nFigure19 âˆ£Revisionmodelexample3. Onthefirstattemptthemodelmakesamistakewiththeformatting\\nof the final answer; it corrects this on the second attempt.\\n31\\nFigure 20 âˆ£Revision model example 4. On the first few attempts the model fails the base 10 to base 8\\nconversion. On the final attempt it makes the correct calculation.\\n32\\nFigure 21 âˆ£Revision model example 5. On the first two attempts the model makes an error when\\nconverting euclidean to polar coordinates. On the final attempt it does not make these mistakes.\\n33\\nFigure 22 âˆ£Revision model example 6. On the first two attempts the model makes a mistake when\\nsumming the proper divisors of 284. On the third attempt, it evaluates this sum correctly.\\n34\\nFigure 23 âˆ£Revision model example 7. On the first attempt the model evaluates1\\n3+2incorrectly. On the\\nsecond attempt it corrects this error.\\n35\\nFigure 24 âˆ£PRM beam search example 1.\\nFigure 25 âˆ£PRM beam search example 2.\\nFigure 26 âˆ£PRM beam search example 3.\\nFigure 27 âˆ£PRM beam search example 4.\\n36\\nFigure 28 âˆ£PRM beam search example 5.\\nFigure 29 âˆ£PRM beam search example 6.\\n37\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [Document(text=document_text)]"
      ],
      "metadata": {
        "id": "UkQmcI9tsWHA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "gfP2wXf7suYQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 3\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=top_k\n",
        ")"
      ],
      "metadata": {
        "id": "4fjEPG_ns9Yp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)]\n",
        "    )"
      ],
      "metadata": {
        "id": "glm8KSJYtty4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is test time scaling about?\"\n",
        "\n",
        "response = query_engine.query(query)"
      ],
      "metadata": {
        "id": "7L2OcazUuA-D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Context:\\n\"\n",
        "for i in range(top_k):\n",
        "  context += response.source_nodes[i].text"
      ],
      "metadata": {
        "id": "RcazMpEnIwY4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "-iRSYPzhMKl1",
        "outputId": "cd94ea15-8dc2-47d0-d4e7-6d63a99932bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Context:\\nby training a process-based dense verifier [ 22,45] and searching against this\\nverifier), the ability scale test-time compute could be greatly improved, as we show in the paper.\\nTo understand the benefits of scaling up test-time computation, we carry out experiments on the\\nchallenging MATH [ 13] benchmark using PaLM-2 [ 3] models specifically fine-tuned1to either revise\\n1Capability-specific finetuning is necessary to induce revision and verification capabilities into the base model on MATH\\n2\\nincorrect answers [28] (e.g. improving the proposal distribution; Section 6) or verify the correctness of\\nindividual steps in an answer using a process-based reward model (PRM) [ 22,45] (Section 5). With\\nboth approaches, we find that the efficacy of a particular test-time compute strategy depends critically\\non both the nature of the specific problem at hand and the base LLM used. For example, on easier\\nproblems, for which the base LLM can already readily produce reasonable responses, allowing the model\\nto iteratively refine its initial answer by predicting a sequence of N revisions (i.e., modifying the proposal\\ndistribution), may be a more effective use of test-time compute than sampling N independent responses in\\nparallel. On the other hand, with more difficult problems that may require searching over many different\\nhigh-level approaches to solving the problem, re-sampling new responses independently in parallel or\\ndeploying tree-search against a process-based reward model is likely a more effective way to use test-time\\ncomputation. This finding illustrates the need to deploy an adaptive â€œcompute-optimalâ€œ strategy for\\nscaling test-time compute , wherein the specific approach for utilizing test-time compute is selected\\ndepending on the prompt, so as to make the best use of additional computation. We also show that a\\nnotion of question difficulty (Section 4) from the perspective of the base LLM can be used to predict the\\nefficacy of test-time computation, enabling us to practically instantiate this â€˜compute-optimalâ€™ strategy\\ngiven a prompt. By appropriately allocating test-time compute in this way, we are able to greatly improve\\ntest-time compute scaling, surpassing the performance of a best-of-N baseline while only using about 4x\\nless computation with both revisions and search (Sections 5 and 6).On the other hand, in many contemporary self-improvement setups,\\nthat would use test-time compute to improve the model, we would likely generate significantly fewer\\n14\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\nProportional to Inference FLOPs20406080100MATH Difficulty Level Accuracy (%)\\nRevisions\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\nProportional to Inference FLOPs20406080MATH Difficulty Level Accuracy (%)\\nPRM Search\\n1\\n2\\n3\\n4\\n5\\nDifficulty Level\\nPretraining Compute Test-time Compute R >> 1 R ~= 1 R << 1Comparing Test-time and Pretraining ComputeFigure 9 âˆ£Tradeoff between pretraining and test-time compute in a FLOPs-matched evaluation. Each line represents the\\nperformance of scaling test-time compute with our compute-optimal policy in each oracle difficulty bin. We plot the results for\\nrevisions on the left and search on the right. The stars represent the greedy pass@1 performance of a base model pretrained\\nwithâˆ¼14times more parameters. We plot test-time compute budget on the x-axis, and place the stars at three different\\nlocations along the x-axis, each corresponding to the FLOPs equivalent point of comparison between scaling parameters and\\nscaling test-time compute for three different inference compute loads (e.g. ğ‘…=ğ·inference\\nğ·pretrain). If the star is below the line, this\\nimplies that it is more effective to use test-time compute than to scale model parameters, and if the star is above the line this\\nimplies that scaling parameters is more effective. We see that on the easy questions or in settings with a lower inference load\\n(e.g.ğ‘…<<1), test-time compute can generally outperform scaling model parameters. However, on the harder questions or in\\nsettings with a higher inference load (e.g. ğ‘…>>1), pretraining is a more effective way to improve performance.\\ninference tokens than pretraining tokens, giving ğ‘…<<1. Therefore, since the scale of test-time compute\\nwe can apply is dependent on this ratio, we expect differing conclusions depending on the specific setting.These sorts of conflicting\\nfindings motivate the need for a systematic analysis of different approaches for scaling test-time compute.\\nCorresponding author(s): csnell22@berkeley.eduarXiv:2408.03314v1  [cs.LG]  6 Aug 2024\\n21232527\\nGeneration Budget202530354045MATH Accuracy (%)\\nCompute Optimal Revisions\\nMajority\\nBest-of-N Weighted\\nCompute Optimal\\nParallel\\n<<1 ~=1 >>1\\nRatio of Inference Tokens to Pretraining Tokens40\\n30\\n20\\n10\\n0102030Relative Improvement in Accuracy\\nFrom Test-time Compute (%)+21.6%\\n+16.7%\\n+5.4%+27.8%\\n+3.5%\\n-24.3%+11.8%\\n-11.9%\\n-37.2%Comparing Test-time and Pretraining Compute\\nin a FLOPs Matched Evauation\\nEasy Questions\\nMedium Questions\\nHard QuestionsIteratively Revising Answers at Test-time\\n21\\n23\\n25\\n27\\n29\\nGeneration Budget1015202530354045MATH Accuracy (%)\\nCompute Optimal Search\\nMajority\\nORM Best-of-N Weighted\\nPRM Best-of-N Weighted\\nPRM Compute Optimal\\n<<1 ~=1 >>1\\nRatio of Inference Tokens to Pretraining Tokens50\\n40\\n30\\n20\\n10\\n01020Relative Improvement in Accuracy\\nFrom Test-time Compute (%)+19.1%\\n+2.2% +2.0%\\n-5.6%\\n-35.6%-30.6%0.0%\\n-35.3%\\n-52.9%Comparing Test-time and Pretraining Compute\\nin a FLOPs Matched Evauation\\nEasy Questions\\nMedium Questions\\nHard QuestionsTest-time Search Against a PRM VerifierFigure 1 âˆ£Summary of our main results. Left: Compute-optimal scaling for iterative self-refinement (i.e., revisions) and search. On\\nthe left, we compare the compute-optimal scaling policy for our PaLM 2-S* revision model against baselines in the revision setting (top) and the\\nPRM search setting (bottom). We see that in the revisions case, the gap between standard best-of-N (e.g.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sFPj4JUfMQm6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"Your an AI assistant. Your goal is to provide answers given the provided relevant context . Use the context if it is relevant to the question. If you don't know the answer, just say you don't know.\n",
        "{context}\n",
        "\n",
        "Please respond to the following question:\"\"\"\n",
        "\n",
        "message_template = [\n",
        "    {\"role\":\"system\", \"content\": system_prompt},\n",
        "    {\"role\":\"user\", \"content\": query}\n",
        "]\n",
        "\n",
        "input = tokenizer.apply_chat_template(message_template)\n",
        "\n",
        "input = torch.tensor([input]).to(device)"
      ],
      "metadata": {
        "id": "ljEs8QXt_Tb2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90d7e541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a6cc68-b4df-47cb-8fa0-7a81996b6933"
      },
      "source": [
        "output = model.generate(input, max_length=4096)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tokenizer.decode(output[0])"
      ],
      "metadata": {
        "id": "FY25SwVXMn5h"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = decoded.find(\"[/INST]\")\n",
        "\n",
        "decoded[start:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "zxbirBLRNDmL",
        "outputId": "eb27db36-e6d0-434f-bf3f-56bff3a217f6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[/INST] Test time scaling refers to the process of improving the performance of a language model at test time by utilizing additional computation. This can be achieved through various strategies such as iteratively refining initial answers by predicting a sequence of revisions or sampling new responses independently in parallel or deploying tree-search against a process-based reward model. The specific approach for utilizing test-time compute depends on the nature of the problem and the base language model used. The findings from the research suggest that an adaptive \"compute-optimal\" strategy is necessary to make the best use of additional computation and improve test-time compute scaling.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}