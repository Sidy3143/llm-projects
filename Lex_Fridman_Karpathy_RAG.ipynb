{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzuPpxKfeYDLHuUYfjusJK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidy3143/llm-projects/blob/main/Lex_Fridman_Karpathy_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a RAG system using the Mistral-7B model to answer questions based on the Lex Fridman and Andrej Karpathy podcast transcript.**"
      ],
      "metadata": {
        "id": "RbXaTqd932eI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Mistal-7B model with quantized 4-bit"
      ],
      "metadata": {
        "id": "pGbqCVbQrH3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fsspec==2025.3.0 gcsfs datasets transformers trl peft bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "QVWz-lPYAyZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\""
      ],
      "metadata": {
        "id": "cbqQP-P7yZNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login # you need token access to mistral model from hugging face"
      ],
      "metadata": {
        "id": "0PwU4Op8bwy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "tZB0c2ajK1fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "wQIixi2-byKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_compute_dtype=\"bfloat16\",\n",
        "                                bnb_4bit_use_double_quant=True,\n",
        "                                )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map = {\"\":0},\n",
        ")"
      ],
      "metadata": {
        "id": "rctPB_PQfyw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load transcript of the Lex fridman-Andrej Karpathy podcast"
      ],
      "metadata": {
        "id": "9lBbEIdCrj8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf8erYP4qREZ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://podscript.ai/podcasts/lex-fridman-podcast/333-andrej-karpathy-tesla-ai-self-driving-optimus-aliens-and-agi'\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "metadata": {
        "id": "4H1FTp5Hzh57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all div classes\n",
        "for div in soup.find_all('div'):\n",
        "    print(div.get('class'))"
      ],
      "metadata": {
        "id": "P9-PUhA8aTf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_div = soup.find('div', class_='post-content') # 'post-content' is the one containning the actual transcript\n",
        "\n",
        "transcript_text = transcript_div.get_text(separator='\\n') #extract the text\n",
        "\n",
        "transcript_text"
      ],
      "metadata": {
        "id": "3UPV782vZa5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We clip the unnecessary parts in the intro (sponsors etc.) and the weird text at the bottom (episode info)."
      ],
      "metadata": {
        "id": "Oiljs9mOsCZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_marker = \"And now, dear friends, hereâ€™s Andrej Karpathy.\"\n",
        "end_marker = \"\\nEpisode Info\"\n",
        "\n",
        "\n",
        "start_idx = transcript_text.find(start_marker)\n",
        "end_idx = transcript_text.find(end_marker)\n",
        "\n",
        "\n",
        "cleaned_transcript =  transcript_text[start_idx + len(start_marker):end_idx]\n",
        "\n",
        "\n",
        "cleaned_transcript"
      ],
      "metadata": {
        "id": "s9Y-ynJAd8cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to a text file\n",
        "with open('karpathy_lex_transcript_cleaned.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(cleaned_transcript)\n",
        "\n",
        "print(\"Transcript saved successfully!\")"
      ],
      "metadata": {
        "id": "7INRs0fUqeJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a264430"
      },
      "source": [
        "!pip install -q -U langchain_community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "TZrPpxdyU7WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain_huggingface"
      ],
      "metadata": {
        "id": "ab73C8aMgd6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e73bef8"
      },
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "langchain_embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # you can choose different embedding models from hugging face"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu # or faiss-gpu if using gpu"
      ],
      "metadata": {
        "id": "xdZawo_m6fTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ff17be"
      },
      "source": [
        "# Customize text splitting with conversational separators\n",
        "text_splitter_conv = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=25,\n",
        "    separators=[\"\\nLex Fridman\\n\\n\\n\", \"\\nAndrej Karpathy\\n\\n\\n\"] # this is the format for who is talking, so we devide by it to not mix\n",
        ")\n",
        "\n",
        "chunks_conv = text_splitter_conv.split_text(cleaned_transcript)\n",
        "\n",
        "print(f\"{len(chunks_conv)} chunks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_documents = [Document(page_content=chunk) for chunk in chunks_conv]"
      ],
      "metadata": {
        "id": "WH36b75tC9k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_vectorstore = FAISS.from_documents(faiss_documents, langchain_embedder)\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "-IoNvSMzm5Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without RAG"
      ],
      "metadata": {
        "id": "N2zqezSS1qgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"Answer the following question appropriately.\"\n",
        "\n",
        "query = \"What did Andrej karpathy learn from Elon Musk?\"\n",
        "\n",
        "message_template = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "\n",
        "input = tokenizer.apply_chat_template(message_template)\n",
        "\n",
        "input = torch.tensor([input]).to(model.device)\n",
        "\n",
        "output = model.generate(input, max_length=1024)\n",
        "\n",
        "decoded = tokenizer.decode(output[0])"
      ],
      "metadata": {
        "id": "L0eDc-imjhDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded"
      ],
      "metadata": {
        "id": "wapj-IlJz-7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives a good answer but not specific to what Andrej said during the conversation. So let's give it some context with the FAISS retriever."
      ],
      "metadata": {
        "id": "F-MlG8iI1tl_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c68e332"
      },
      "source": [
        "# Now try retrieving with the FAISS retriever\n",
        "query = \"What did Andrej karpathy learn from Elon Musk?\"\n",
        "\n",
        "faiss_results = faiss_retriever.invoke(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Context:\\n\"\n",
        "for i in range(5):\n",
        "  context += faiss_results[i].page_content"
      ],
      "metadata": {
        "id": "m2bzZg2EzXZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "id": "4yzKBb7TzwtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"Your an AI assistant. Your goal is to provide answers about the lex fridman and Andrej Karpathy podcasts, given the provided relevant context with timestamps. Use the context if it is relevant to the question. If you don't know the answer, just say you don't know.\n",
        "{context}\n",
        "\n",
        "Please respond to the following question:\"\"\""
      ],
      "metadata": {
        "id": "sCaN5I8VuxJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_template = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "\n",
        "input = tokenizer.apply_chat_template(message_template)"
      ],
      "metadata": {
        "id": "sBsljbs-tnuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([input]).to(model.device)\n",
        "\n",
        "output = model.generate(input, max_length=1024)\n",
        "\n",
        "decoded = tokenizer.decode(output[0])"
      ],
      "metadata": {
        "id": "AO9U9DGDueqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_response = decoded.find(\"[/INST]\") # only show what the model generated\n",
        "\n",
        "response = decoded[start_response:]\n",
        "response"
      ],
      "metadata": {
        "id": "24rpCD9L0JAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gives an answer very specific and accurate to what Andrej said during the episode."
      ],
      "metadata": {
        "id": "scWv6p1O0hx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Can also use **llama Index**"
      ],
      "metadata": {
        "id": "44iqGfdjwYDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install llama-index\n",
        "!pip -q install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "97Ee4d72waUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.schema import Document\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "RGl6iavGwfIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\") # alternative model\n",
        "\n",
        "Settings.llm = None\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 25"
      ],
      "metadata": {
        "id": "ua8WujB9wfCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [Document(text=cleaned_transcript)]"
      ],
      "metadata": {
        "id": "Yyj6Zm-eyI1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "CyRXKujNyIuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 3\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=top_k,\n",
        ")"
      ],
      "metadata": {
        "id": "LEkhMfLuyR3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
        ")"
      ],
      "metadata": {
        "id": "hz_38_BmyRng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did Andrej karpathy learn from Elon Musk?\"\n",
        "response = query_engine.query(query)"
      ],
      "metadata": {
        "id": "G4HizJe0yXfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Context:\\n\"\n",
        "for i in range(top_k):\n",
        "  context += response.source_nodes[i].text + \"\\n\\n\""
      ],
      "metadata": {
        "id": "2phJYd7Dybxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "id": "acdCXLg-04fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"Your an AI assistant. Your goal is to provide answers about the lex fridman and Andrej Karpathy podcasts, given the provided relevant context with timestamps. Use the context if it is relevant to the question. If you don't know the answer, just say you don't know.\n",
        "{context}\n",
        "\n",
        "Please respond to the following question:\"\"\""
      ],
      "metadata": {
        "id": "fvYJLrm3kdvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_template = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "\n",
        "input_tokens = tokenizer.apply_chat_template(message_template)"
      ],
      "metadata": {
        "id": "5Pe9bWJ5p_M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([input_tokens]).to(model.device)\n",
        "\n",
        "output = model.generate(input, max_length=4096)\n",
        "\n",
        "decoded = tokenizer.decode(output[0])"
      ],
      "metadata": {
        "id": "pcsQ__m8zdre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded"
      ],
      "metadata": {
        "id": "Jf2uaTV6lFTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "marker_output = decoded.find(\"[/INST]\") # only extract the model's answer\n",
        "\n",
        "output = decoded[marker_output:]"
      ],
      "metadata": {
        "id": "DS_s-KHGltRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "WZKcIv4mq3PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we get a good answer related to what he said in the podcast."
      ],
      "metadata": {
        "id": "iTIkfpsD3hHW"
      }
    }
  ]
}